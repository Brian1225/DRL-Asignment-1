  0%|                                                                                                                              | 0/10000 [00:00<?, ?it/s]/home/chunyulin/Desktop/DRL/DRL-Asignment-1/utils.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(state, dtype=torch.float32).to(self.device),
/home/chunyulin/Desktop/DRL/DRL-Asignment-1/utils.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(next_state, dtype=torch.float32).to(self.device),
 32%|████████████████████████████████████▊                                                                              | 3199/10000 [09:35<20:43,  5.47it/s]
Episode: 100, Average reward: -200.94999999999985, Avg Loss: 132.66577228441835, Epsilon: 0.951217530242334
Episode: 200, Average reward: -199.34999999999985, Avg Loss: 206.87093319416047, Epsilon: 0.9048147898403269
Episode: 300, Average reward: -187.74999999999986, Avg Loss: 206.68861214876176, Epsilon: 0.8606756897186528
Episode: 400, Average reward: -182.29999999999984, Avg Loss: 207.41618295133114, Epsilon: 0.8186898039137951
Episode: 500, Average reward: -165.89999999999986, Avg Loss: 216.76538173675536, Epsilon: 0.7787520933134615
Episode: 600, Average reward: -157.1499999999999, Avg Loss: 210.48721904933453, Epsilon: 0.7407626428726788
Episode: 700, Average reward: -163.14999999999986, Avg Loss: 196.52609871506692, Epsilon: 0.7046264116491338
Episode: 800, Average reward: -154.1999999999999, Avg Loss: 178.65940500855447, Epsilon: 0.6702529950324074
Episode: 900, Average reward: -146.09999999999988, Avg Loss: 178.34864314973353, Epsilon: 0.637556398572254
Episode: 1000, Average reward: -135.9999999999999, Avg Loss: 178.94706076264382, Epsilon: 0.606454822840097
Episode: 1100, Average reward: -132.34999999999994, Avg Loss: 176.9745311242342, Epsilon: 0.5768704587855094
Episode: 1200, Average reward: -128.49999999999991, Avg Loss: 172.28953392028808, Epsilon: 0.548729293075715
Episode: 1300, Average reward: -120.19999999999993, Avg Loss: 160.6110261410475, Epsilon: 0.5219609229311034
Episode: 1400, Average reward: -114.39999999999995, Avg Loss: 162.36126573443414, Epsilon: 0.49649837999353363
Episode: 1500, Average reward: -110.29999999999993, Avg Loss: 165.21372260481118, Epsilon: 0.4722779627867691
Episode: 1600, Average reward: -102.49999999999996, Avg Loss: 169.2136377903819, Epsilon: 0.44923907734991153
Episode: 1700, Average reward: -105.84999999999995, Avg Loss: 170.9583250311017, Epsilon: 0.4273240856451275
Episode: 1800, Average reward: -98.19999999999996, Avg Loss: 162.09098434403538, Epsilon: 0.406478161360422
Episode: 1900, Average reward: -92.94999999999996, Avg Loss: 168.5391732467711, Epsilon: 0.3866491527467055
Episode: 2000, Average reward: -87.49999999999999, Avg Loss: 167.3510808160901, Epsilon: 0.3677874521460121
Episode: 2100, Average reward: -86.19999999999999, Avg Loss: 155.11352050229908, Epsilon: 0.34984587188445015
Episode: 2200, Average reward: -80.41299999999997, Avg Loss: 154.98369823098182, Epsilon: 0.3327795262194029
Episode: 2300, Average reward: -76.3, Avg Loss: 173.22053408175708, Epsilon: 0.31654571904563433
Episode: 2400, Average reward: -78.39999999999999, Avg Loss: 163.56671363711357, Epsilon: 0.3011038370793723
Episode: 2500, Average reward: -75.25, Avg Loss: 144.43685488894582, Epsilon: 0.28641524825313086
Episode: 2600, Average reward: -69.2, Avg Loss: 140.2029906706512, Epsilon: 0.27244320506708813
Episode: 2700, Average reward: -65.417, Avg Loss: 134.62895984046162, Epsilon: 0.25915275265522114
Episode: 2800, Average reward: -62.30000000000001, Avg Loss: 155.56340720713138, Epsilon: 0.24651064133620196
Episode: 2900, Average reward: -60.350000000000016, Avg Loss: 158.20979487776756, Epsilon: 0.23448524343027585
Episode: 3000, Average reward: -63.75000000000002, Avg Loss: 140.3510889005661, Epsilon: 0.22304647413401948
Episode: 3100, Average reward: -58.40000000000002, Avg Loss: 141.10330542474986, Epsilon: 0.21216571625502262
Episode: 3200, Average reward: -59.10000000000001, Avg Loss: 142.108113604486, Epsilon: 0.2018157486181985
Episode: 3300, Average reward: -51.05000000000002, Avg Loss: 139.13311746843158, Epsilon: 0.1919706779646106
Episode: 3400, Average reward: -50.20000000000002, Avg Loss: 128.6882600093633, Epsilon: 0.1826058741724434
Episode: 3500, Average reward: -49.45000000000002, Avg Loss: 129.12819375246764, Epsilon: 0.17369790863805412
Episode: 3600, Average reward: -48.600000000000016, Avg Loss: 125.86699769213796, Epsilon: 0.1652244956629483
Episode: 3700, Average reward: -47.600000000000016, Avg Loss: 129.91961034536362, Epsilon: 0.1571644367000449
Episode: 3800, Average reward: -48.50000000000002, Avg Loss: 136.24815862912683, Epsilon: 0.1494975673197442
Episode: 3900, Average reward: -43.15000000000002, Avg Loss: 132.4737536931783, Epsilon: 0.1422047067631241
Episode: 4000, Average reward: -46.25000000000002, Avg Loss: 127.64002384681254, Epsilon: 0.13526760995605422
Episode: 4100, Average reward: -45.40000000000002, Avg Loss: 126.06431775845587, Epsilon: 0.12866892186418147
Episode: 4200, Average reward: -39.45000000000002, Avg Loss: 123.02308212645352, Epsilon: 0.12239213407459068
Episode: 4300, Average reward: -39.55000000000002, Avg Loss: 121.39674971550703, Epsilon: 0.1164215434955208
Episode: 4400, Average reward: -124.27999999999993, Avg Loss: 183.1108205671981, Epsilon: 0.11074221307080971
Episode: 4500, Average reward: -215.04999999999984, Avg Loss: 406.41517416715624, Epsilon: 0.10533993441078586
Episode: 4600, Average reward: -220.84499999999986, Avg Loss: 373.7205666637421, Epsilon: 0.1002011922461172
Episode: 4700, Average reward: -214.29999999999984, Avg Loss: 341.0954290127754, Epsilon: 0.09531313061568897
Episode: 4800, Average reward: -218.79999999999984, Avg Loss: 345.9458201313019, Epsilon: 0.09066352070392071
Episode: 4900, Average reward: -215.94999999999985, Avg Loss: 329.9049996805191, Epsilon: 0.08624073024705824
Episode: 5000, Average reward: -219.49999999999986, Avg Loss: 327.8108256196976, Epsilon: 0.0820336944319021
Episode: 5100, Average reward: -214.44999999999985, Avg Loss: 318.86611657619477, Epsilon: 0.07803188821416818
Episode: 5200, Average reward: -214.89999999999986, Avg Loss: 325.4432315444946, Epsilon: 0.07422529998722693
Episode: 5300, Average reward: -215.94999999999985, Avg Loss: 331.31572670698165, Epsilon: 0.0706044065353464
Episode: 5400, Average reward: -214.68099999999984, Avg Loss: 336.6432139992714, Epsilon: 0.06716014920877787
Episode: 5500, Average reward: -214.69999999999985, Avg Loss: 340.4153642988205, Epsilon: 0.06388391126108031
Episode: 5600, Average reward: -215.34999999999985, Avg Loss: 334.47907698631286, Epsilon: 0.06076749629198526
Episode: 5700, Average reward: -216.44999999999985, Avg Loss: 325.2500558924675, Epsilon: 0.05780310774187243
Episode: 5800, Average reward: -215.09999999999985, Avg Loss: 319.4325326943397, Epsilon: 0.05498332938655545
Episode: 5900, Average reward: -218.74799999999985, Avg Loss: 318.0295710372925, Epsilon: 0.05230110678358006
Episode: 6000, Average reward: -218.99999999999986, Avg Loss: 326.88086738944054, Epsilon: 0.04974972962361759
Traceback (most recent call last):
  File "/home/chunyulin/Desktop/DRL/DRL-Asignment-1/train.py", line 134, in <module>
    main()
  File "/home/chunyulin/Desktop/DRL/DRL-Asignment-1/train.py", line 130, in main
    trainer.train(args)
  File "/home/chunyulin/Desktop/DRL/DRL-Asignment-1/train.py", line 75, in train
    self.memory.push(state, action, reward, next_state, done)
  File "/home/chunyulin/Desktop/DRL/DRL-Asignment-1/utils.py", line 48, in push
    torch.tensor(action, dtype=torch.long).to(self.device),
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/chunyulin/Desktop/DRL/DRL-Asignment-1/train.py", line 134, in <module>
    main()
  File "/home/chunyulin/Desktop/DRL/DRL-Asignment-1/train.py", line 130, in main
    trainer.train(args)
  File "/home/chunyulin/Desktop/DRL/DRL-Asignment-1/train.py", line 75, in train
    self.memory.push(state, action, reward, next_state, done)
  File "/home/chunyulin/Desktop/DRL/DRL-Asignment-1/utils.py", line 48, in push
    torch.tensor(action, dtype=torch.long).to(self.device),
KeyboardInterrupt
